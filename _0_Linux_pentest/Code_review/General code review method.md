# Types of tools
## Static analysis
Add linters, etc
in ALC: unit tests and continuous dev
AVA: not necessarily practical to rebuild or run linters on codebase 

## Software composition analysis
more dedicated to detect dependencies, identify CVEs, etc
-> SNYK
in ALC: can be part of ALC_FLR
AVA: for the public vulnerability analysis!

## Dynamic analysis
see debugging / code instrumentation (e.g. ThreadSanitizer for data races)
in ALC/ATE: unit tests and dev functional testing
AVA: probably impractical on real-life sized products, also instrumentation is not always possible/practical, but other methods possible e.g. fuzzing


# Integration in CC eval
Doxa: 
 - In CC, functional testing **is** penetration testing
 - consequences on tools used: 
     - SAST/DAST: they are meant to be DevOps and **should** be implemented by the developer itself in ALC/ATE. The evaluator merely reviews them. **BUT** if the developer hasn't done it, the evaluator **should** step in and use standard tools:
         - SAST, e.g. for compliance to
             - BSI TR-02102: Codyze not mature yet (**false positives**, only part 1 covered...)
             - CERT C L1 rules: [my general-purpose C code review method](_0_Linux_pentest\Code_review\C_code_review_VAN3.md), but also 
         - DAST [TODO]
     - SCA: The evaluator **shall** perform SCA for CVE analysis
         - it is not explicitly said, but using SCA on open-souce libraries means that **we limit the AVA_VAN.3 activities on open source parts to their CVE analysis** (We could not check the code on a real-life prduct otherwise - too big)
 - CVE analysis provides inputs that have to be analysed:
     - DoS is not relevant in CC
     - do not consider CVEs with preconditions that do not reflect AGD, issues that can be fixed be rebooting the TOE, etc
     - as a general rule, consider a CVE as an observation report written by a third-party: to be discussed/confirmed w/ the developer
     - severity scores are useless (do not take into account our TOE/TSF)
 - Fuzzing:
     - Most of the time, it does not really work out because it is grey-box at most, rather than white-box (code may be available, but the test case does not really build upon the knowledge of the design, although it is available in a CC evaluation) **BUT** being unstructured is also its virtue: they may spot errors that are notoriously hard to spot manually, such as off-by-one, integer overflows, pointer arithmetic issues, etc.
     - in a typical EAL4, fuzzing can only be limited to black-box on a simple protocol from the TOE

Practical issues:
 - [QUESTION:] (for the Fachexpert): can SCA even work on proprietary code. i.e. code that was forked from an antique BSD in 1876 and heavily modified ever since? what do we do in these cases? should we consider everything proprietary code?
 - [QUESTION:] (for the Fachexpert/certifier): If severity is useless, we have to consider all CVEs -> how is that even possible? (there are too many!). We cannot even remove DoS CVEs becasue they could have been mislabelled as DoS, and be way worse
 - [QUESTION:] (for the Fachexpert/certifier): SCA is only as good as the CVE databases. Should we simply require up-to-date libraries instead of spending an absurd amount of hours based on such a shaky foundation? (whatever "up-to-date" means is another question)
 - [QUESTION:] (for the Fachexpert/certifier): no (serious) SAST/DAST by developers in many evaluations -> can we leverage a CC requirement to force them do it? Can we even demand a clean build?
 - [QUESTION:] (for the certifier): "functional testing in CC IS penetration testing" -> in reality, only ATE_COV.3 hints at negative/boundary testing. if the dev has only bullshit positive testing, the evaluator has to  do everything in ATE_IND, which does not include this activity - or does that mean that 
     - *ATE_IND.2.3E The evaluator shall test a subset of the TSF to confirm that the TSF operates as specified.* is independant from
     - *ATE_IND.2.2E The evaluator shall execute a sample of tests in the test documentation to verify the developer test results.* and that it includes more tests than the sample
 - [QUESTION:] what about the BSI document about fuzzing? applicable? 
 - 